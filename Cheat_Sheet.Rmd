 ---
title: "RMarkdown, Simplified"
author: "Author:  Srivastav Budugutta"
output: html_document
---

# FrameWorks and Method 1 Cheat Sheet {.tabset}

## Basic Codes {.tabset}

### R code {.tabset}

#### Working with R {.tabset}

##### Data Structure
vector: one-dimensional, all objects belong to same class
list: laundry-bag type flexible structure that can contain objects of different classes
matrix: two-dimensional structure, all objects belong to same class
data.frame: two-dimensional structure, where each column contains objects of the same class

```{r setup}
# 2 vector multiplication
vec = c(1,2,3,4)
vec * c(1,2)
```

Matrix: 

```{r}
matrix_1 = matrix(1:8,nrow=2,ncol=4)
matrix_2 = matrix(8:1,nrow=2,ncol=4)
matrix_1*matrix_2

```
List


```{r}
name_and_age = list('Nikhil', 8, "Rohan", 10)
class(name_and_age)

```

Data Frame

```{r}
set.seed(10)
df = data.frame(id = 1:10, 
           gender=sample(c('Male','Female'),size = 10,replace = T),
           attended=sample(c(T,F),size = 10,replace=T),
           score=sample(x = 1:100,size = 10,replace = T), stringsAsFactors = T)
df

```

Reprex function is used to formating output in comment form




```{r}

library(reprex)
reprex(mean(c(1,2,NA,4)),advertise = F)
```

##### Calculating Mode

Most frequent value. Base R doesnâ€™t have a built-in function for mode, but we can write a function to do it.

```{r}
library(tidyverse)
mode_function = function(dat){
  unique_value_list = unique(dat)
  print(match(dat, unique_value_list))
  print(tabulate(match(dat, unique_value_list)))
  unique_value_list[which.max(tabulate(match(dat, unique_value_list)))]
}
mode_function(c(1,1,2,3,4,1,5,6,2,2,2,2,2,2,2,100))

```
##### Summary calculation 

```{r}
mpg%>%
  summarize(Min = min(hwy),
            FirstQuartile = quantile(hwy,0.25), # number of variables below 25% population
            Median = median(hwy),
            Mean = mean(hwy),
            ThirdQuartile = quantile(hwy,0.75), # number of variables below 75% population
            Max = max(hwy),
            Range = max(hwy) - min(hwy),
            SD = sd(hwy),
            Variance = var(hwy))


```
##### Using Table

This give the count of each variable type. (Used for categorical variable)

```{r}

table(mpg$cyl)
prop.table(table(mpg$cyl))

#  using Xtab
xtabs(~cyl,data = mpg)

# using metrix
# Levels of a categorical variable are often compared based on descriptive measure of a numerical variable in what is known as a crosstab. E.g., average gas mileage of cars for each category of cylinder. Here are a few different ways to do this.

tapply(X = mpg$hwy,INDEX = mpg$cyl,FUN = 'mean')
```

##### Displaying the Data in pretty format

```{r}
library(skimr)
skim(mpg)
library(summarytools)
print(dfSummary(mpg,style='grid',graph.col = T),method = 'render')

```


##### Interactive table


```{r}

library(DT)
datatable(mpg)

# last five rows
tail(mpg,n = 5)

```


##### Subset


```{r}

names(mpg)
mpg[,"manufacturer"]
head(mpg)
mpg[mpg$displ>2,c("manufacturer","model")]

# arrange it in ascending order
mpg_subset = mpg[mpg$displ>2,c("manufacturer","model","year")]
mpg_subset
mpg_subset[order(mpg_subset$year, decreasing  = F),]

# Using Dplyr
mpg%>%
  filter(displ>2) %>%
  select(manufacturer,model,year)%>%
  arrange(year)


# another subset

day_of_week = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday')  
number_of_smartwatches_sold = c(25,20,15,20,30,80,90)  
price_per_smartwatch = c(200,200,200,200,200,150,150)  
df = data.frame(day_of_week,number_of_smartwatches_sold,price_per_smartwatch)

df
df$price_per_smartwatch[df$day_of_week == "Saturday"]

# another one
olympics_swimming = read.csv('data/olympics_swimming.csv')
median(na.omit(olympics_swimming$height[olympics_swimming$year==1960]))
median(na.omit(olympics_swimming$height[olympics_swimming$year==2016 & olympics_swimming$medalist == "Yes"]))

```


##### Visual Summary {.tabset}

###### Histogram

```{r}

ggplot(data=mpg,mapping = aes(x=hwy))+
  geom_histogram()

hist(mpg$hwy)

# using binwidth
ggplot(data=mpg,mapping = aes(x=hwy))+
  geom_histogram(binwidth = 2)

# comparing yearly
ggplot(data=mpg,mapping = aes(x=hwy,fill=factor(year)))+
  geom_histogram()

# frequency polygon
ggplot(data=mpg,mapping = aes(x=hwy,color=factor(year)))+
  geom_freqpoly(size=1.2)

ggplot(data=mpg,aes(x=hwy,color=factor(drv)))+
  geom_freqpoly(size=1.2)

# The issue of unequal sample sizes can be addressed by comparing standardized frequency distributions called density curves.

ggplot(data=mpg,aes(x=hwy,color=factor(drv)))+
  geom_density(size=1.2)
```
###### BoxPlot
A box plot (also known as a box and whisker plot) is another handy chart for examining a distribution. However, it is not as popular as a histogram for examining distribution of a single variable. On the other hand, these plots are useful for comparing distributions. They are also useful for spotting outliers.

```{r}
ggplot(data=mpg,aes(x="",y=hwy))+
  geom_boxplot()

ggplot(data=mpg,aes(x=factor(year),y=hwy))+
  geom_boxplot()

ggplot(data=mpg,aes(x=factor(year),y=hwy))+
  geom_boxplot(outlier.color='red', fill='cadetblue',color='sienna')+
  geom_text(data=mpg[mpg$hwy>quantile(mpg$hwy,0.99),], aes(label=manufacturer),nudge_x = 0.2)
```
###### QQ Plot
Most commonly used to see if data comes from a normal distribution by plotting quantiles of data against quantiles expected from a normal distribution. If the data are normally distributed the points in the plot will cluster around the diagonal running from bottom left to top right. Lets examine distribution of hwy and then examine it for each drive train, drv.
```{r}
ggplot(data=mpg, aes(sample=hwy))+
  geom_qq_line()+
  geom_qq()

ggplot(data=mpg, aes(sample=hwy, color=drv))+
  geom_qq_line()+
  geom_qq()
```

###### Bar Chart
Typically used to compare summary of numerical variables across levels of a categorical variable. Bar charts are created using the geom, geom_bar. To get familiar with the workings of geom_bar, let us create a bar chart that compares the number of cars of each drive train, drv. Since the goal is to compare counts, we donâ€™t need to specify a numerical variable.


```{r}
ggplot(data=mpg, aes(x=drv, y=hwy, fill=drv))+
  geom_bar(stat='summary', fun='median', show.legend=F)
ggplot(data=mpg, aes(x=drv, y=hwy, fill=drv))+
  geom_bar(stat='summary', fun='median', show.legend=F)+
  coord_flip()

ggplot(data=mpg,aes(x=factor(drv),fill=factor(year),y=hwy))+
  geom_bar(stat = 'summary', fun='mean', position='dodge')
```

###### Scatter Plot

```{r}
ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()


g1 = ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()+ggtitle('color aesthetic')
g2 = ggplot(data=mpg,aes(x=displ,y=cty,size=factor(year)))+
  geom_point()+ggtitle('size aesthetic')
g3 = ggplot(data=mpg,aes(x=displ,y=cty,alpha=factor(year)))+
  geom_point()+ggtitle('alpha aesthetic')
g4 = ggplot(data=mpg,aes(x=displ,y=cty,shape=factor(year)))+
  geom_point()+ggtitle('shape aesthetic')
library(gridExtra)
chart = grid.arrange(g1,g2,g3,g4)
```

###### Line Graph

With large number of points and overplotting, spotting a trend in a scatterplot can be difficult. A line graph can clearly depict the trend in the data.

```{r}

ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()+
  geom_smooth(method='lm',se=F,size=1.2)
```


###### Facet
Facets create a grid of smaller plots that display different subsets of the data
```{r}

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(.~cyl)

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(cyl~.)

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_wrap(~cyl)

# Grid
# Lastly, we can facet two different variables by including them as x and y axes in facet_grid(). In the example below, we add a fourth variable, year to facet_grid().

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(cyl~year)
```


##### Arithmetic operations


```{r}
#log base 5
log(390625, base = 5)

cos(0)

## Type of data structure:
# vector because if there is a character in it all the remaining elements will be converted to character
c("I am smart", TRUE, 1000)

```

#### Prediction Algorithms {.tabset}

##### Decision Tree {.tabset}

**Where/when to use trees**

using large number of independent variables and

when there are likely to be non-linear relationships and interactions amongst
variables.

And when interpretability of results is important

**Issues**
perform poorly in predicting relative to the best supervised learning
approaches
tend to overfit the data


In decision tree the predictor space is divided into segments or it will be stratified and the predictions is made based on the summary of from that particular region.

**Process Of Spliting the data**

- The process begins with the entire data, R, and searches every distinct value of every predictor to find the predictor and split value that partitions the data into two groups (R1 and R2) such that the overall sum of squared errors is minimized.

- Next, the process is repeated, looking for the best predictor and best split value (or cut point) in order to split the data further so as to minimize the SSE within each of the resulting regions.
- However, this time, instead of splitting the entire predictor space, one of the previously identified regions is split. This creates three regions, R1, R2 and R3.Because of the recursive splitting nature of trees, this method is also known as recursive partitioning (and inspiration for the name of the R library, `rpart`).

- This recursive partitioning process continues until a stopping criterion is reached such as number of observations in a region falls below 5.

**Short Comes**

- It is computationally infeasible to consider every possible partition of the feature
space into J boxes.

- This is the reason for the top-down, greedy approach of recursive binary splitting.

- The approach is top-down because it begins at the top of the tree and then
successively splits the predictor space; each split is indicated via two new branches
further down on the tree.

- It is greedy because at each step of the tree-building process, the best split is made
at that particular step, rather than looking ahead and picking a split that will lead to
a better tree in some future step.

**Predictor Importance**

- Relative importance of predictors in trees can be determined by examining the
reduction in SSE attributed to each split.

- Intuitively, predictors that appear higher in the tree, or that appear multiple times in
the tree will be more important than predictors that appear lower in the tree or not
at all

**Summary**
1) Use recursive binary splitting to grow a large tree on the training data, stopping only when
each terminal node has fewer than some minimum number of observations.
2) Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees,
as a function of cp.
3) Use k-fold cross-validation to choose Î±. For each k = 1, . . . , K:
    a) Repeat Steps 1 and 2 on the ð¾âˆ’1/ð¾ th fraction of the training data, excluding the kth fold.
    b) Evaluate the mean squared prediction error on the data in the left-out kth fold, as a
function of cp.

    c) Average the results, and pick cp to minimize the average error.
    
4. Return the subtree from Step 2 that corresponds to the chosen value of cp
. 

A set of *segment rules* is used to segment the predictor space.

Decision Tree can be used for both predicting continuous outcome and categorical outcome.

It partitions the data into smaller group that has greater proportion of one of the categories of the outcome.

CART (classification and regression Tree methodology) is oldest and widely being used.

**Predictors include categorical and continuous variables: Trees can handle both categorical and continuous predictors Categorical variables can be handled automatically, without dummy coding.**

**Trees handles the predictors even if they are skewed. No transformation of predictors are required nor they need to be standardized. No need to create interaction term to explore join effect of 2 or more predictors**
Joint effect here refers to how other variables include the outcome. Like for instance if a customer is purchasing the product then combination of age and income are the factors that influence the outcome variable. Decision tree will automatically handles these.

**Missing values are automatically handled by decision tree by using surrogate splits.**

Surrogate splits means if we have income which are having some missing values, decision tree will use surrogate split are created using other variables which are correlated with income like experience, job title etc.. this is only used as backup if primary split cannot be made on income due to missing data.

**Weakness of Trees**
Less-than optimal predictive performance.
â€“ Tree models partition the predictor space into rectangular regions. If the
relationship between predictors and outcome is not adequately described by
these rectangles, performance will be poor.
â€“ Number of possible predicted outcomes is finite and is determined by the
number of leaves. This limitation is unlikely to capture all of the nuances of the
data.

Model instability
â€“ Slight changes in the data can drastically change the structure of the tree,
hence the interpretation. Thus, these models have high variance. Easy to overfit train data





###### Regression Tree Continuous


###### Calssification Tree Categorical

**Loading Data**

```{r}
# Loading the Data
library(ISLR2)
data(Credit)
library(dplyr)
Credit2 = 
  Credit %>%
  mutate(Balance_hilo = as.integer(Balance > median(Balance,na.rm = T)))%>%
  mutate(Balance_hilo = factor(Balance_hilo, labels = c('low','high'),ordered = T))%>%
  select(-Balance)

```

**Splitting Data**

```{r}
library(caret)
set.seed(1031)
split = createDataPartition(y = Credit2$Balance_hilo, p = 0.75, list = F)
train = Credit2[split,]
test = Credit2[-split,]

```

**Estimate**

For instance, for a 35 year old, one can navigate through the tree to arrive at a predicted probability of 0.43 for a â€œhighâ€ balance. 

```{r}
library(rpart)
library(rpart.plot)
tree1 = rpart(Balance_hilo~Age,data = train, method = 'class')
rpart.plot(tree1)

```

**Predict**

```{r}
predict(tree1, newdata = data.frame(Age = 35), type='class')
predict(tree1, newdata = data.frame(Age = 35), type='prob')[,'high'] > 0.5

```

##### Tuning the Tree {.tabset}

**Advantages**

1) Trees are very easy to explain to people. In fact, they are even easier to explain than
linear regression!

2) Some people believe that decision trees more closely mirror human decision-making
than do regression and classification approaches.

3) Trees can be displayed graphically, and are easily interpreted even by a non-expert
(especially if they are small).

4) Trees can easily handle qualitative predictors without the need to create dummy
variables.

**Disadvantages And its Solution**

1) Trees are prone to overfitting train data

Sol: we can use cross validation to optimize the complexity and pick good model hyper parameter.


2) Unfortunately, trees generally do not have the same level of predictive accuracy as
some of the other prediction and classification approaches. 

Sol: The prediction can be very much improved by aggregating multiple decision trees.

**Tuning HyperParameter**
Parameters are estimated during model training and hyper parameter controls the learning process 

Process:

1) Identify the parameters to be tuned
  mostly we will choose CP (complexity parameter which will control the size of the tree)
  


2) Specify the range of hyperparametrs to evaluate and this process is called grid search

For a regression tree, say 41 values of cp from 0 to 0.16 in intervals of 0.004.
  Values to evaluate may be informed by experience. Alternatively, one can use a
trial and error approach. E.g., begin with a broad grid search (e.g., intervals of
0.01) and narrow it down later to focus on a specific range (e.g., intervals of
0.001). why 0.004 is because when we divide 0.16 by 0.004 we will have 40 attributes to evaluate


3) Train the model on each combination of hyperparameter

4) measure performance

Training error is a poor guide for test error
â€“ Test sample can only be used for evaluating the final model, not a
series of models with different combinations of hyperparameters
â€“ Solution is to use a resampling procedure called cross-validation

5) choose the hyper parameter which performs the best.
Select the hyperparameter values for the model with lowest crossvalidation error

**Cross Validation Error**

To conclude at optimal complexity we have 2 approches:
1) apply penality on training error rate to estimate test error rate eg: AIC, BIC etc..

2) Cross-validation


Cross validation is a general technique called resampling. Another method is bootstraping which is part of re-sampling technique.

Resampling methods involve repeatedly drawing samples from a training set and refitting a model
of interest on each sample in order to obtain more information about the fitted model
â€“ Model Assessment: estimate test error rates
â€“ Model Selection: select the appropriate level of model flexibility

**Various Cross-validation methods are:**

1. validation set approach
    * Randomly split the data into a estimation set and a validation set. (split the data randomly into 2 halfs).
  
    * Estimate the model using the estimation sample.
  
    * Apply the estimated model to the validation sample
  
    * Error in the validation-set provides an estimate of error in a new sample. 
    * **Goods and Issues with this approach**
        * Simple approach.
        * Not computationally intensive
        * The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.
        * In the validation approach, only a subset of the observations â€” those that
are included in the training set rather than in the validation set â€” are used to
fit the model. This suggests that the validation set error may tend to
overestimate the test error for the model fit on the entire data set. 

1. k-fold cross-validation
    + Idea is to randomly divide the data into k equal-sized parts. We leave
out part k, fit the model to the other k âˆ’ 1 parts (combined), and then
obtain predictions for the left-out kth part.
    + This is done in turn for each part k = 1, 2, . . . K, and then the results
are combined.
    + Let the K parts be C1, C2, . . . CK, where Ck denotes the indices of the observations in part k. There are nk observations in part k: if N is a multiple of K, then nk = n/K. 
    + When k = n, we get n-fold or leave-one-out cross validation
    + ** Evaludation**
        + Less bias than Validation because model is trained on a larger sample
(for 5-fold, 80% compared to 50% for Validation)
        + Less variance because test statistic is averaged k times.
        + Computationally more intensive than Validation but less than LOOCV
        + Works best for k=5 or k=10


1. leave one out cross-validation (LOOCV)
    + Randomly divide the train sample into two halves:
        1. Estimation Sample: n-1
        1. Validation Sample: 1
    + Fit the model on estimation sample
    + Test model on validation sample
    + Repeat Steps 1-3, n times
    + MSE is average of n models.
    + **Evaluation**
        + Less bias because almost the entire sample is used for training the model
        + Generally less variance than Validation because of n repeats.
        + LOOCV is computationally intensive
            + mostly because algorithms use the same approach as k-fold cross validation instead of using a computational shortcut.
        + LOOCV sometimes useful, but typically doesnâ€™t shake up the data enough.
        + The estimates from each fold are highly correlated and hence their average
can have high variance.
        + A better choice is k = 5 or 10. 


##### Esemble Models {.tabset}

**About Ensemble Model**

Ensemble models take on a wisdom of the crowds approach by
combining predictions from a number of models.

1. Two heads are better than one, Many heads are even better.

1. Two categories of tree-based ensemble methods
    + Bagging and Forests: Grow multiple trees by using bootstrapped samples and average predictions.
    
    + Boosting: Grow a series of trees sequentially, each tree using information from the previously grown trees
    
**Trees are easy to interpret butâ€¦**

    + They generally underperform regression based methods
    + And they tend to overfit the data
    
**Three ensemble models to improve trees are**

1. Bagging
    + Draw samples from original data with replacement
    + Repeat multiple times
    + **It involves 3 steps**
        + Generating a large number of bootstrapped samples from train sample
        + Train the model on each sample
        + Combining models by averaging (metric outcome) or majority vote (non-metric outcome)
    + **After all the trees have been trained, their predictions are combined by taking the average (for regression problems) or the majority vote (for classification problems) of the predictions from all the trees.**
    + **The bagging process can help to reduce the variance of the model by reducing the impact of outliers or noisy data points, and can also improve the accuracy by reducing overfitting.**
    + **Trees constructed in bagging are not pruned, so they tend to be very large trees. But, that is okay, because we are going to average a large number of trees.**
    + **Averaging predictions reduces variance while leaving bias unchanged.**
    + **Packages:**
        + library(ipred)
        + library(randomForest) (after a small tweak)

1. Random Forests

    + Random forests provide an improvement over bagged trees by way of a small tweak
that de-correlates the trees. This reduces the variance when we average the trees.
    + As in bagging, we build a number of decision trees on bootstrapped training
samples.
    + But when building these decision trees, each time a split in a tree is considered, a
random selection of m predictors is chosen as split candidates from the full set of p
predictors. The split is allowed to use only one of those m predictors.
    + A fresh selection of m predictors is taken at each split, and typically we choose m â‰ˆ
âˆšp â€” that is, the number of predictors considered at each split is approximately
equal to the square root of the total number of predictors
    + Notice that random forest with m = p is same as bagging!!
    + **Why consider a random sample of predictors?**
        + Suppose that we have a very strong predictor in the data set along with a number of other moderately strong predictor, then in the collection of bagged trees, most or all of them will use the very strong predictor for the first split!
        + All bagged trees will look similar. Hence all the predictions from the bagged trees will be highly correlated
        + Averaging many highly correlated quantities does not lead to a large variance reduction, and thus random forests â€œde-correlatesâ€ the bagged trees leading to more reduction in variance.
    + **R packages**
        + library(randomForest)
        + library(ranger)
        + library(Rborist)

1. Boosting
    + Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification.
    + Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.
    + Notably, each tree is built on a bootstrap data set, independent of the other trees.
    +Boosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees.

    + **Approach**
        + Unlike fitting a single large decision tree to the data, which amounts to fitting
the data hard and potentially overfitting, the boosting approach instead learns slowly.
        + Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.
        + Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm.
        + By fitting small trees to the residuals, we slowly improve Ë†f in areas where it
does not perform well. The shrinkage parameter Î» slows the process down even further, allowing more and different shaped trees to attack the residuals.
    + **Tuning Parameters**
        + Choice of hyperparameters is critical in establishing a boosting model. Here are some of the important ones in the Gradient Boosting Machine.
        
            1. n.trees: The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use crossvalidation to select B.
            2. shrinkage: The shrinkage parameter Î», a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can
depend on the problem. Very small Î» can require using a very large value of B in order to
achieve good performance.
            3. interaction.depth: The number of splits d in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump,
consisting of a single split and resulting in an additive model. More generally d is the
interaction depth, and controls the interaction order of the boosted model, since d splits
can involve at most d variables. *
    + **Adv. Dis.**
        + Tend to perform better than state-of-the-art prediction models
        + Some boosting algorithms (e.g., gbm and lightgbm) can optimize a user-defined cost function not just minimize a standard loss function
        + Prone to overfitting. Tuning is extremely important
        + Can be extremely slow
        + Sensitive to extreme values
    + **R packages**
        + library(gbm)
        + library(xgboost)
        + library(lightgbm)



#### Data Tidying  {.tabset}

##### Basic Parsings {.tabset}

```{r}
library(readr)
parse_number('increase in sales of â‚¬123.456,78',locale = locale(grouping_mark = '.'))
```

Parse character:
However, UTF-8 encoding may not be human readable. Would you like ice cream from \x48\xc3\xa4\x61\x67\x65\x6e\x2d\x44\x61\x7a\x73?


```{r}
parse_character('\x48\xc3\xa4\x61\x67\x65\x6e\x2d\x44\x61\x7a\x73')
parse_character("\U0001F4A9")

```
**Dates**

Year %Y (4 digits) %y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999

Month %m (2 digits) %b (abbreviated name, like â€œJanâ€) %B (full name, â€œJanuaryâ€)

Day %d (2 digits). %e (optional leading space)

Time %H 0-23 hour %I 0-12, must be used with %p %p AM/PM indicator %M minutes %S integer seconds %OS real seconds %Z Time zone (as name, e.g. America/Chicago) %z (as offset from UTC, e.g. +0800)

Non-digits %. skips one non-digit character %* skips any number of non-digits
```{r}
as.Date('4/7/1776',format='%d/%m/%Y')
# lubridate provides a number of simple functions to convert date and time to the ISO standard.
library(lubridate)
mdy('7/4/1776')
dmy('4/7/1776')
mdy_hms('July 4, 1776 3:25:13 pm',tz = 'US/Eastern')


```

**Extracting months, days, years once formated**

```{r}
month(mdy('7/4/1776'),label = T)
wday(mdy('7/4/1776'),label = T)

```

**More Example**

```{r}
location = c('USA, north america','China,asia','India, asia', 'Russia, asia','France, europe','UK, europe')
national_day = c('July 4, 1776','October 1, 1949','August 15, 1947','June 12, 1990','July 14, 1789','none')
national_day_alt = c('4/7/1776','1/10/1949','15/8/1947','12/6/1990','14/7/1789','none')
per_capita_dollars = c('$65,280.70','$10,261.70','$2,104.10','$11,585.00','$40,493.90','$42,300.30')
per_capita_euros = c('55.488,59 Euro','8.722,44 Euro','1.788,48 Euro','9.847,25 Euro','34.419,82 Euro','35.955,26 Euro')
messy_data = data.frame(location, national_day, national_day_alt, per_capita_dollars,per_capita_euros)
# The dollar sign provides context to the numbers, however its presence gets in the way of conducting numerical analysis.
messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))

# Like the per_capita_dollars, this variable contains the currency label. Furthermore, the numbers are formatted using a grouping convention common in Europe.

messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))

# Date format used is Month, Date, Year.
messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))%>%
  mutate(national_day = mdy(national_day))

```

**Separate**

```{r}

#An alternative to using regular expressions for extracting country and continent is to split continent based on the comma delimiter. The library tidyr has a function, separate that does just that. We will still use str_to_title to change case for continent. separate adds on some whitespace which is removed using str_trim.

messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))%>%
  mutate(national_day = mdy(national_day))%>%
  mutate(national_day_alt = dmy(national_day_alt))%>%
  separate(col = location,into = c('country','continent'), sep = ',')%>%
  mutate(continent = str_trim(str_to_title(continent)))%>%
  select(country,continent,everything())

```

**Using refactor (fct_recode), Rename, everything()

```{r}
head(messy_data)
data_1 = messy_data %>% 
  mutate(per_capita_dollars = parse_number(per_capita_dollars)) %>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.'))) %>%
  mutate(national_day_alt = dmy(national_day_alt)) %>%
  mutate(national_day = mdy(national_day)) %>%
  separate(col = location, into = c("Country","continent"), sep = ",") %>%
  mutate(continent = str_trim(str_to_title(continent))) %>%
  select(Country, continent, everything()) %>%
  mutate(continent = fct_recode(continent, "West" = "North America", "East" = "Asia", "West" = "Europe")) %>%
  rename('Zone' = "continent")
  
  
  
  
head(data_1)

```


##### Pivoting Data {.tabset}

**Gather function**

```{r}
gdp = data.frame(c('USA','China','Japan'),
           c(543300, 59716, 44307),
           c(19390604, 12237700, 4872136))
names(gdp) = c('Country','1960','2017')
gdp

#Pivot wider

gdp_tall = 
  gdp %>%
  gather('Year','GDP',2:3)
gdp_tall

```

**Pivot Longer**

```{r}
gdp %>% pivot_longer(cols = 2:3, names_to = "Year", values_to = "GDP")

```


**Spread Function**

```{r}
gdp_wide = 
  gdp_tall %>%
  spread('Year','GDP')
gdp_wide

# Pivot wider

gdp_wide = 
  gdp_tall %>%
  pivot_wider(names_from = 'Year',values_from  = 'GDP')
gdp_wide

```

**More Examples** (runif function is used)

```{r}
df = data.frame(id = 1:100,
                c1 = round(runif(100,1,5),0),
                c2 = round(runif(100,1,5),0),
                c3 = round(runif(100,1,7),0),
                c4 = round(runif(100,1,7),0),
                c5 = round(runif(100,1,7),0))
df_1 = gather(df, key = "Coupon", value = 'c', 2:length(df))
df_1

# Creating bar chart
ggplot(data = df_1, mapping = aes(x=Coupon,y=c,fill=Coupon))+
  geom_bar(stat='summary',position='dodge',fun='mean')

## Using piped operation to directly map 
df %>%
  pivot_longer(cols = c1:c5, names_to = 'item',values_to='c')%>%
  ggplot(aes(x=item,y=c,fill=item))+
  geom_bar(stat='summary',position='dodge',fun='mean')
  
```

** More Example with csv**

```{r}
data = read.csv('data/CrimeTrendsInOneVar.csv',header = T,skip = 4,nrows=55)
tail(data)

data %>%
  pivot_longer(cols = Alabama:Wyoming, names_to = 'State', values_to = 'Number_of_Violent_Crimes')%>%
  head()

data_1 = gather(data, key = "State", value = "Number_of_Crimes", Alabama:length(data))
data_1

data %>%
  pivot_longer(cols = Alabama:Wyoming, names_to = 'State', values_to = 'Number_of_Violent_Crimes')%>%
  group_by(State)%>%
  summarize(AverageViolentCrime = mean(Number_of_Violent_Crimes,na.rm=T))%>%
  ggplot(aes(x=reorder(State,X = AverageViolentCrime), y=AverageViolentCrime,fill=AverageViolentCrime))+
  geom_col()+scale_fill_continuous(low='white',high='red')+xlab('State')+ylab('Crime')+
  theme(axis.text.y = element_text(size = 6, hjust = .5, vjust = .5, face = "plain"))+
  coord_flip()
```



### Python code {.tabset}

#### Data Structure

```{python}
import numpy as np
arr = np.array([[1, 2, 3], 
               [4, 5, 6]])
print(arr)

# to cret dimensions of array:
a = np.array(42)
d = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])
print(a.ndim)
print(d.ndim)
```


create n dimension array

Access 2-D Arrays


```{python}
# create n dimension array:
arr = np.array([1, 2, 3, 4], ndmin=5)
print(arr)



# Access 2-D Arrays
arr = np.array([[1,2,3,4,5], [6,7,8,9,10]])

print('2nd element on 1st row: ', arr[0, 1])
arr
arr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
arr
print(arr[0, 1, 2])

```
Data types in Numpy:

i - integer
b - boolean
u - unsigned integer
f - float
c - complex float
m - timedelta
M - datetime
O - object
S - string
U - unicode string
V - fixed chunk of memory for other type ( void )

```{python}
arr = np.array(['apple', 'banana', 'cherry'])

print(arr.dtype)


```

Copy vs New

The copy owns the data and any changes made to the copy will not affect original array, and any changes made to the original array will not affect the copy.

The view does not own the data and any changes made to the view will affect the original array, and any changes made to the original array will affect the view.

```{python}
arr = np.array([1, 2, 3, 4, 5])
x = arr.copy()
arr[0] = 42

print(arr)
print(x)

# View


arr = np.array([1, 2, 3, 4, 5])
x = arr.view()
arr[0] = 42

print(arr)
print(x)

```

Get Shape of Array

```{python}
arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

print(arr.shape)

```

Reshape 1D to 2D

```{python}
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

newarr = arr.reshape(4, 3)

print(newarr)

```

Reshape 1D to 3D


```{python}

arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

newarr = arr.reshape(2, 3, 2)

print(newarr)
```

Unknown Dimension

You are allowed to have one "unknown" dimension.

Meaning that you do not have to specify an exact number for one of the dimensions in the reshape method.

Pass -1 as the value, and NumPy will calculate this number for you.


```{python}
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

newarr = arr.reshape(2, 2, -1)

print(newarr)

```
Flattening the arrays



```{python}
arr = np.array([[1, 2, 3], [4, 5, 6]])

newarr = arr.reshape(-1)

print(newarr)

```

ndenumerate()


```{python}
arr = np.array([1, 2, 3])

for idx, x in np.ndenumerate(arr):
  print(idx, x)
  
arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

for idx, x in np.ndenumerate(arr):
  print(idx, x)

```




```{python}


```



