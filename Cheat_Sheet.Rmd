 ---
title: "RMarkdown, Simplified"
author: "Author:  Srivastav Budugutta"
output: html_document
---

# FrameWorks and Method 1 Cheat Sheet {.tabset}

## Basic Codes {.tabset}

### R code {.tabset}

#### Working with R {.tabset}

##### Data Structure
vector: one-dimensional, all objects belong to same class
list: laundry-bag type flexible structure that can contain objects of different classes
matrix: two-dimensional structure, all objects belong to same class
data.frame: two-dimensional structure, where each column contains objects of the same class

```{r setup}
# 2 vector multiplication
vec = c(1,2,3,4)
vec * c(1,2)
```

Matrix: 

```{r}
matrix_1 = matrix(1:8,nrow=2,ncol=4)
matrix_2 = matrix(8:1,nrow=2,ncol=4)
matrix_1*matrix_2

```
List


```{r}
name_and_age = list('Nikhil', 8, "Rohan", 10)
class(name_and_age)

```

Data Frame

```{r}
set.seed(10)
df = data.frame(id = 1:10, 
           gender=sample(c('Male','Female'),size = 10,replace = T),
           attended=sample(c(T,F),size = 10,replace=T),
           score=sample(x = 1:100,size = 10,replace = T), stringsAsFactors = T)
df

```

Reprex function is used to formating output in comment form




```{r}

library(reprex)
reprex(mean(c(1,2,NA,4)),advertise = F)
```

##### Calculating Mode

Most frequent value. Base R doesnâ€™t have a built-in function for mode, but we can write a function to do it.

```{r}
library(tidyverse)
mode_function = function(dat){
  unique_value_list = unique(dat)
  print(match(dat, unique_value_list))
  print(tabulate(match(dat, unique_value_list)))
  unique_value_list[which.max(tabulate(match(dat, unique_value_list)))]
}
mode_function(c(1,1,2,3,4,1,5,6,2,2,2,2,2,2,2,100))

```
##### Summary calculation 

```{r}
mpg%>%
  summarize(Min = min(hwy),
            FirstQuartile = quantile(hwy,0.25), # number of variables below 25% population
            Median = median(hwy),
            Mean = mean(hwy),
            ThirdQuartile = quantile(hwy,0.75), # number of variables below 75% population
            Max = max(hwy),
            Range = max(hwy) - min(hwy),
            SD = sd(hwy),
            Variance = var(hwy))


```
##### Using Table

This give the count of each variable type. (Used for categorical variable)

```{r}

table(mpg$cyl)
prop.table(table(mpg$cyl))

#  using Xtab
xtabs(~cyl,data = mpg)

# using metrix
# Levels of a categorical variable are often compared based on descriptive measure of a numerical variable in what is known as a crosstab. E.g., average gas mileage of cars for each category of cylinder. Here are a few different ways to do this.

tapply(X = mpg$hwy,INDEX = mpg$cyl,FUN = 'mean')
```

##### Displaying the Data in pretty format

```{r}
library(skimr)
skim(mpg)
library(summarytools)
print(dfSummary(mpg,style='grid',graph.col = T),method = 'render')

```


##### Interactive table


```{r}

library(DT)
datatable(mpg)

# last five rows
tail(mpg,n = 5)

```


##### Subset


```{r}

names(mpg)
mpg[,"manufacturer"]
head(mpg)
mpg[mpg$displ>2,c("manufacturer","model")]

# arrange it in ascending order
mpg_subset = mpg[mpg$displ>2,c("manufacturer","model","year")]
mpg_subset
mpg_subset[order(mpg_subset$year, decreasing  = F),]

# Using Dplyr
mpg%>%
  filter(displ>2) %>%
  select(manufacturer,model,year)%>%
  arrange(year)


# another subset

day_of_week = c('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday')  
number_of_smartwatches_sold = c(25,20,15,20,30,80,90)  
price_per_smartwatch = c(200,200,200,200,200,150,150)  
df = data.frame(day_of_week,number_of_smartwatches_sold,price_per_smartwatch)

df
df$price_per_smartwatch[df$day_of_week == "Saturday"]

# another one
olympics_swimming = read.csv('data/olympics_swimming.csv')
median(na.omit(olympics_swimming$height[olympics_swimming$year==1960]))
median(na.omit(olympics_swimming$height[olympics_swimming$year==2016 & olympics_swimming$medalist == "Yes"]))

```


##### Visual Summary {.tabset}

###### Histogram

```{r}

ggplot(data=mpg,mapping = aes(x=hwy))+
  geom_histogram()

hist(mpg$hwy)

# using binwidth
ggplot(data=mpg,mapping = aes(x=hwy))+
  geom_histogram(binwidth = 2)

# comparing yearly
ggplot(data=mpg,mapping = aes(x=hwy,fill=factor(year)))+
  geom_histogram()

# frequency polygon
ggplot(data=mpg,mapping = aes(x=hwy,color=factor(year)))+
  geom_freqpoly(size=1.2)

ggplot(data=mpg,aes(x=hwy,color=factor(drv)))+
  geom_freqpoly(size=1.2)

# The issue of unequal sample sizes can be addressed by comparing standardized frequency distributions called density curves.

ggplot(data=mpg,aes(x=hwy,color=factor(drv)))+
  geom_density(size=1.2)
```
###### BoxPlot
A box plot (also known as a box and whisker plot) is another handy chart for examining a distribution. However, it is not as popular as a histogram for examining distribution of a single variable. On the other hand, these plots are useful for comparing distributions. They are also useful for spotting outliers.

```{r}
ggplot(data=mpg,aes(x="",y=hwy))+
  geom_boxplot()

ggplot(data=mpg,aes(x=factor(year),y=hwy))+
  geom_boxplot()

ggplot(data=mpg,aes(x=factor(year),y=hwy))+
  geom_boxplot(outlier.color='red', fill='cadetblue',color='sienna')+
  geom_text(data=mpg[mpg$hwy>quantile(mpg$hwy,0.99),], aes(label=manufacturer),nudge_x = 0.2)
```
###### QQ Plot
Most commonly used to see if data comes from a normal distribution by plotting quantiles of data against quantiles expected from a normal distribution. If the data are normally distributed the points in the plot will cluster around the diagonal running from bottom left to top right. Lets examine distribution of hwy and then examine it for each drive train, drv.
```{r}
ggplot(data=mpg, aes(sample=hwy))+
  geom_qq_line()+
  geom_qq()

ggplot(data=mpg, aes(sample=hwy, color=drv))+
  geom_qq_line()+
  geom_qq()
```

###### Bar Chart
Typically used to compare summary of numerical variables across levels of a categorical variable. Bar charts are created using the geom, geom_bar. To get familiar with the workings of geom_bar, let us create a bar chart that compares the number of cars of each drive train, drv. Since the goal is to compare counts, we donâ€™t need to specify a numerical variable.


```{r}
ggplot(data=mpg, aes(x=drv, y=hwy, fill=drv))+
  geom_bar(stat='summary', fun='median', show.legend=F)
ggplot(data=mpg, aes(x=drv, y=hwy, fill=drv))+
  geom_bar(stat='summary', fun='median', show.legend=F)+
  coord_flip()

ggplot(data=mpg,aes(x=factor(drv),fill=factor(year),y=hwy))+
  geom_bar(stat = 'summary', fun='mean', position='dodge')
```

###### Scatter Plot

```{r}
ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()


g1 = ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()+ggtitle('color aesthetic')
g2 = ggplot(data=mpg,aes(x=displ,y=cty,size=factor(year)))+
  geom_point()+ggtitle('size aesthetic')
g3 = ggplot(data=mpg,aes(x=displ,y=cty,alpha=factor(year)))+
  geom_point()+ggtitle('alpha aesthetic')
g4 = ggplot(data=mpg,aes(x=displ,y=cty,shape=factor(year)))+
  geom_point()+ggtitle('shape aesthetic')
library(gridExtra)
chart = grid.arrange(g1,g2,g3,g4)
```

###### Line Graph

With large number of points and overplotting, spotting a trend in a scatterplot can be difficult. A line graph can clearly depict the trend in the data.

```{r}

ggplot(data=mpg,aes(x=displ,y=cty,color=factor(year)))+
  geom_point()+
  geom_smooth(method='lm',se=F,size=1.2)
```


###### Facet
Facets create a grid of smaller plots that display different subsets of the data
```{r}

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(.~cyl)

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(cyl~.)

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_wrap(~cyl)

# Grid
# Lastly, we can facet two different variables by including them as x and y axes in facet_grid(). In the example below, we add a fourth variable, year to facet_grid().

ggplot(data=mpg,aes(x=displ,y=hwy))+
  geom_point()+
  facet_grid(cyl~year)
```


##### Arithmetic operations


```{r}
#log base 5
log(390625, base = 5)

cos(0)

## Type of data structure:
# vector because if there is a character in it all the remaining elements will be converted to character
c("I am smart", TRUE, 1000)

```

#### Prediction Algorithms {.tabset}

##### Decision Tree {.tabset}

**Where/when to use trees**

using large number of independent variables and

when there are likely to be non-linear relationships and interactions amongst
variables.

And when interpretability of results is important

**Issues**
perform poorly in predicting relative to the best supervised learning
approaches
tend to overfit the data


In decision tree the predictor space is divided into segments or it will be stratified and the predictions is made based on the summary of from that particular region.

**Process Of Spliting the data**

- The process begins with the entire data, R, and searches every distinct value of every predictor to find the predictor and split value that partitions the data into two groups (R1 and R2) such that the overall sum of squared errors is minimized.

- Next, the process is repeated, looking for the best predictor and best split value (or cut point) in order to split the data further so as to minimize the SSE within each of the resulting regions.
- However, this time, instead of splitting the entire predictor space, one of the previously identified regions is split. This creates three regions, R1, R2 and R3.Because of the recursive splitting nature of trees, this method is also known as recursive partitioning (and inspiration for the name of the R library, `rpart`).

- This recursive partitioning process continues until a stopping criterion is reached such as number of observations in a region falls below 5.

**Short Comes**

- It is computationally infeasible to consider every possible partition of the feature
space into J boxes.

- This is the reason for the top-down, greedy approach of recursive binary splitting.

- The approach is top-down because it begins at the top of the tree and then
successively splits the predictor space; each split is indicated via two new branches
further down on the tree.

- It is greedy because at each step of the tree-building process, the best split is made
at that particular step, rather than looking ahead and picking a split that will lead to
a better tree in some future step.

**Predictor Importance**

- Relative importance of predictors in trees can be determined by examining the
reduction in SSE attributed to each split.

- Intuitively, predictors that appear higher in the tree, or that appear multiple times in
the tree will be more important than predictors that appear lower in the tree or not
at all

**Summary**
1) Use recursive binary splitting to grow a large tree on the training data, stopping only when
each terminal node has fewer than some minimum number of observations.
2) Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees,
as a function of cp.
3) Use k-fold cross-validation to choose Î±. For each k = 1, . . . , K:
    a) Repeat Steps 1 and 2 on the ð¾âˆ’1/ð¾ th fraction of the training data, excluding the kth fold.
    b) Evaluate the mean squared prediction error on the data in the left-out kth fold, as a
function of cp.

    c) Average the results, and pick cp to minimize the average error.
    
4. Return the subtree from Step 2 that corresponds to the chosen value of cp
. 

A set of *segment rules* is used to segment the predictor space.

Decision Tree can be used for both predicting continuous outcome and categorical outcome.

It partitions the data into smaller group that has greater proportion of one of the categories of the outcome.

CART (classification and regression Tree methodology) is oldest and widely being used.

**Predictors include categorical and continuous variables: Trees can handle both categorical and continuous predictors Categorical variables can be handled automatically, without dummy coding.**

**Trees handles the predictors even if they are skewed. No transformation of predictors are required nor they need to be standardized. No need to create interaction term to explore join effect of 2 or more predictors**
Joint effect here refers to how other variables include the outcome. Like for instance if a customer is purchasing the product then combination of age and income are the factors that influence the outcome variable. Decision tree will automatically handles these.

**Missing values are automatically handled by decision tree by using surrogate splits.**

Surrogate splits means if we have income which are having some missing values, decision tree will use surrogate split are created using other variables which are correlated with income like experience, job title etc.. this is only used as backup if primary split cannot be made on income due to missing data.

**Weakness of Trees**
Less-than optimal predictive performance.
â€“ Tree models partition the predictor space into rectangular regions. If the
relationship between predictors and outcome is not adequately described by
these rectangles, performance will be poor.
â€“ Number of possible predicted outcomes is finite and is determined by the
number of leaves. This limitation is unlikely to capture all of the nuances of the
data.

Model instability
â€“ Slight changes in the data can drastically change the structure of the tree,
hence the interpretation. Thus, these models have high variance. Easy to overfit train data





###### Regression Tree Continuous


###### Calssification Tree Categorical

**Loading Data**

```{r}
# Loading the Data
library(ISLR2)
data(Credit)
library(dplyr)
Credit2 = 
  Credit %>%
  mutate(Balance_hilo = as.integer(Balance > median(Balance,na.rm = T)))%>%
  mutate(Balance_hilo = factor(Balance_hilo, labels = c('low','high'),ordered = T))%>%
  select(-Balance)

```

**Splitting Data**

```{r}
library(caret)
set.seed(1031)
split = createDataPartition(y = Credit2$Balance_hilo, p = 0.75, list = F)
train = Credit2[split,]
test = Credit2[-split,]

```

**Estimate**

For instance, for a 35 year old, one can navigate through the tree to arrive at a predicted probability of 0.43 for a â€œhighâ€ balance. 

```{r}
library(rpart)
library(rpart.plot)
tree1 = rpart(Balance_hilo~Age,data = train, method = 'class')
rpart.plot(tree1)

```

**Predict**

```{r}
predict(tree1, newdata = data.frame(Age = 35), type='class')
predict(tree1, newdata = data.frame(Age = 35), type='prob')[,'high'] > 0.5

```

##### Tuning the Tree {.tabset}

Test

#### Data Tidying  {.tabset}

##### Basic Parsings {.tabset}

```{r}
library(readr)
parse_number('increase in sales of â‚¬123.456,78',locale = locale(grouping_mark = '.'))
```

Parse character:
However, UTF-8 encoding may not be human readable. Would you like ice cream from \x48\xc3\xa4\x61\x67\x65\x6e\x2d\x44\x61\x7a\x73?


```{r}
parse_character('\x48\xc3\xa4\x61\x67\x65\x6e\x2d\x44\x61\x7a\x73')
parse_character("\U0001F4A9")

```
**Dates**

Year %Y (4 digits) %y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999

Month %m (2 digits) %b (abbreviated name, like â€œJanâ€) %B (full name, â€œJanuaryâ€)

Day %d (2 digits). %e (optional leading space)

Time %H 0-23 hour %I 0-12, must be used with %p %p AM/PM indicator %M minutes %S integer seconds %OS real seconds %Z Time zone (as name, e.g. America/Chicago) %z (as offset from UTC, e.g. +0800)

Non-digits %. skips one non-digit character %* skips any number of non-digits
```{r}
as.Date('4/7/1776',format='%d/%m/%Y')
# lubridate provides a number of simple functions to convert date and time to the ISO standard.
library(lubridate)
mdy('7/4/1776')
dmy('4/7/1776')
mdy_hms('July 4, 1776 3:25:13 pm',tz = 'US/Eastern')


```

**Extracting months, days, years once formated**

```{r}
month(mdy('7/4/1776'),label = T)
wday(mdy('7/4/1776'),label = T)

```

**More Example**

```{r}
location = c('USA, north america','China,asia','India, asia', 'Russia, asia','France, europe','UK, europe')
national_day = c('July 4, 1776','October 1, 1949','August 15, 1947','June 12, 1990','July 14, 1789','none')
national_day_alt = c('4/7/1776','1/10/1949','15/8/1947','12/6/1990','14/7/1789','none')
per_capita_dollars = c('$65,280.70','$10,261.70','$2,104.10','$11,585.00','$40,493.90','$42,300.30')
per_capita_euros = c('55.488,59 Euro','8.722,44 Euro','1.788,48 Euro','9.847,25 Euro','34.419,82 Euro','35.955,26 Euro')
messy_data = data.frame(location, national_day, national_day_alt, per_capita_dollars,per_capita_euros)
# The dollar sign provides context to the numbers, however its presence gets in the way of conducting numerical analysis.
messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))

# Like the per_capita_dollars, this variable contains the currency label. Furthermore, the numbers are formatted using a grouping convention common in Europe.

messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))

# Date format used is Month, Date, Year.
messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))%>%
  mutate(national_day = mdy(national_day))

```

**Separate**

```{r}

#An alternative to using regular expressions for extracting country and continent is to split continent based on the comma delimiter. The library tidyr has a function, separate that does just that. We will still use str_to_title to change case for continent. separate adds on some whitespace which is removed using str_trim.

messy_data %>%
  mutate(per_capita_dollars = parse_number(per_capita_dollars))%>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.')))%>%
  mutate(national_day = mdy(national_day))%>%
  mutate(national_day_alt = dmy(national_day_alt))%>%
  separate(col = location,into = c('country','continent'), sep = ',')%>%
  mutate(continent = str_trim(str_to_title(continent)))%>%
  select(country,continent,everything())

```

**Using refactor (fct_recode), Rename, everything()

```{r}
head(messy_data)
data_1 = messy_data %>% 
  mutate(per_capita_dollars = parse_number(per_capita_dollars)) %>%
  mutate(per_capita_euros = parse_number(per_capita_euros, locale = locale(grouping_mark = '.'))) %>%
  mutate(national_day_alt = dmy(national_day_alt)) %>%
  mutate(national_day = mdy(national_day)) %>%
  separate(col = location, into = c("Country","continent"), sep = ",") %>%
  mutate(continent = str_trim(str_to_title(continent))) %>%
  select(Country, continent, everything()) %>%
  mutate(continent = fct_recode(continent, "West" = "North America", "East" = "Asia", "West" = "Europe")) %>%
  rename('Zone' = "continent")
  
  
  
  
head(data_1)

```


##### Pivoting Data {.tabset}

**Gather function**

```{r}
gdp = data.frame(c('USA','China','Japan'),
           c(543300, 59716, 44307),
           c(19390604, 12237700, 4872136))
names(gdp) = c('Country','1960','2017')
gdp

#Pivot wider

gdp_tall = 
  gdp %>%
  gather('Year','GDP',2:3)
gdp_tall

```

** Pivot Longer**

```{r}
gdp %>% pivot_longer(cols = 2:3, names_to = "Year", values_to = "GDP")

```


**Spread Function**

```{r}
gdp_wide = 
  gdp_tall %>%
  spread('Year','GDP')
gdp_wide

# Pivot wider

gdp_wide = 
  gdp_tall %>%
  pivot_wider(names_from = 'Year',values_from  = 'GDP')
gdp_wide

```

** More Examples** (runif function is used)

```{r}
df = data.frame(id = 1:100,
                c1 = round(runif(100,1,5),0),
                c2 = round(runif(100,1,5),0),
                c3 = round(runif(100,1,7),0),
                c4 = round(runif(100,1,7),0),
                c5 = round(runif(100,1,7),0))
df_1 = gather(df, key = "Coupon", value = 'c', 2:length(df))
df_1

# Creating bar chart
ggplot(data = df_1, mapping = aes(x=Coupon,y=c,fill=Coupon))+
  geom_bar(stat='summary',position='dodge',fun='mean')

## Using piped operation to directly map 
df %>%
  pivot_longer(cols = c1:c5, names_to = 'item',values_to='c')%>%
  ggplot(aes(x=item,y=c,fill=item))+
  geom_bar(stat='summary',position='dodge',fun='mean')
  
```

** More Example with csv**

```{r}
data = read.csv('data/CrimeTrendsInOneVar.csv',header = T,skip = 4,nrows=55)
tail(data)

data %>%
  pivot_longer(cols = Alabama:Wyoming, names_to = 'State', values_to = 'Number_of_Violent_Crimes')%>%
  head()

data_1 = gather(data, key = "State", value = "Number_of_Crimes", Alabama:length(data))
data_1

data %>%
  pivot_longer(cols = Alabama:Wyoming, names_to = 'State', values_to = 'Number_of_Violent_Crimes')%>%
  group_by(State)%>%
  summarize(AverageViolentCrime = mean(Number_of_Violent_Crimes,na.rm=T))%>%
  ggplot(aes(x=reorder(State,X = AverageViolentCrime), y=AverageViolentCrime,fill=AverageViolentCrime))+
  geom_col()+scale_fill_continuous(low='white',high='red')+xlab('State')+ylab('Crime')+
  theme(axis.text.y = element_text(size = 6, hjust = .5, vjust = .5, face = "plain"))+
  coord_flip()
```



### Python code {.tabset}

#### Data Structure

```{python}
import numpy as np
arr = np.array([[1, 2, 3], 
               [4, 5, 6]])
print(arr)

# to cret dimensions of array:
a = np.array(42)
d = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])
print(a.ndim)
print(d.ndim)
```


create n dimension array

Access 2-D Arrays


```{python}
# create n dimension array:
arr = np.array([1, 2, 3, 4], ndmin=5)
print(arr)



# Access 2-D Arrays
arr = np.array([[1,2,3,4,5], [6,7,8,9,10]])

print('2nd element on 1st row: ', arr[0, 1])
arr
arr = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
arr
print(arr[0, 1, 2])

```
Data types in Numpy:

i - integer
b - boolean
u - unsigned integer
f - float
c - complex float
m - timedelta
M - datetime
O - object
S - string
U - unicode string
V - fixed chunk of memory for other type ( void )

```{python}
arr = np.array(['apple', 'banana', 'cherry'])

print(arr.dtype)


```

Copy vs New

The copy owns the data and any changes made to the copy will not affect original array, and any changes made to the original array will not affect the copy.

The view does not own the data and any changes made to the view will affect the original array, and any changes made to the original array will affect the view.

```{python}
arr = np.array([1, 2, 3, 4, 5])
x = arr.copy()
arr[0] = 42

print(arr)
print(x)

# View


arr = np.array([1, 2, 3, 4, 5])
x = arr.view()
arr[0] = 42

print(arr)
print(x)

```

Get Shape of Array

```{python}
arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

print(arr.shape)

```

Reshape 1D to 2D

```{python}
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

newarr = arr.reshape(4, 3)

print(newarr)

```

Reshape 1D to 3D


```{python}

arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])

newarr = arr.reshape(2, 3, 2)

print(newarr)
```

Unknown Dimension

You are allowed to have one "unknown" dimension.

Meaning that you do not have to specify an exact number for one of the dimensions in the reshape method.

Pass -1 as the value, and NumPy will calculate this number for you.


```{python}
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])

newarr = arr.reshape(2, 2, -1)

print(newarr)

```
Flattening the arrays



```{python}
arr = np.array([[1, 2, 3], [4, 5, 6]])

newarr = arr.reshape(-1)

print(newarr)

```

ndenumerate()


```{python}
arr = np.array([1, 2, 3])

for idx, x in np.ndenumerate(arr):
  print(idx, x)
  
arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])

for idx, x in np.ndenumerate(arr):
  print(idx, x)

```




```{python}


```



